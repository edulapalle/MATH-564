---
title: 
- MATH564 Project
- Predicting Medical Insurance Charges
author: 
- Santosh Reddy Edulapalle, A20501739
- Zainab Hasnain, A20516879
date: "2022-11-09"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading data:

```{r}
df = read.csv("expenses.csv")
#factorising columns
df$sex = as.factor(df$sex)
df$smoker = as.factor(df$smoker)
df$region = as.factor(df$region)
head(df)
#data description
str(df)
```
Visualisation:

```{r}
library(corrplot)
#correlation Matrix
df.corr <- df[,c("age","bmi","children","charges")]
#df.corr
cor_mat <- cor(df.corr)
# correlation graph
corrplot(cor_mat,method = 'number')
```
Visulaising features to target (charges).
```{r}
plot(charges~.,data = df)

```
As we can see from the above 6 graphs, the we can clearly find the relation between these features and the target variable insurance charges.
Key observations:
1. the charges are increasing proportional to the age of the person.
2. Median charges is same for both male and female,but male observations have more Q3 and max whisker size indicating a border expense range.
3. Observations with BMI of around 20-40 have least expense of < 15000 also to note that there are some outliers in these BMI range also.
4. Number of children is inversely proportional to the charges.
5. Smoker-yes observations have very high insurance charges compared to non-smokers.
6. Regional wise, all the regions have same median charges but comparitevly northwest and southwest regions have low wiskers than others.


```{r}
#data splitting 80-20
library(caTools)
set.seed(108)
split <- sample.split(df$charges,SplitRatio = 0.8) #charges is the target variable.
df.train <- subset(df,split == T)
df.test <- subset(df,split == F)
```

# linear regression

```{r}
#linear regression model.
model.lm <- lm(charges ~., data = df.train)
summary(model.lm)
```

```{r}
#predicitng new results
y_pred.lm <- predict(model.lm, newdata = df.test)
```

```{r}
#plotting the testing results
library(ggplot2)
ggplot()+ #observation points
geom_point(aes(x = df.test$charges, y = y_pred.lm),colour = 'red') + # regression line
#since we already made regression equation , we don't need to change the regression line graph.
#geom_line(aes(x = df.train$charges, y = predict(model.lm, newdata = df.train)),colour = 'blue') +
ggtitle('y_true vs y_pred: Test data') +
xlab('y_true') +
ylab('y_pred')
```

# glm

```{r}
#model
model.glm <- glm(formula = charges ~., family = gaussian,data = df.train)
#summary
summary(model.glm)
```


```{r}
#predicitng new results
y_pred.glm <- predict(model.glm, newdata = df.test)
```


```{r}
#plotting the testing results
library(ggplot2)
ggplot()+ #observation points
geom_point(aes(x = df.test$charges, y = y_pred.glm),colour = 'red') + # regression line
#since we already made regression equation , we don't need to change the regression line graph.
#geom_line(aes(x = df.train$charges, y = predict(model.lm, newdata = df.train)),colour = 'blue') +
ggtitle('y_true vs y_pred: Test data') +
xlab('y_true') +
ylab('y_pred')
```